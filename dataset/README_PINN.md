# Furnace Physics-Informed Neural Network (PINN) Training

Production-ready PyTorch training script for modeling an industrial furnace using Physics-Informed Neural Networks.

## ğŸ¯ Overview

This PINN learns to predict:
- **Outlet Temperature** (T_out)
- **Excess O2** (O2)

Based on 6 operational inputs while simultaneously learning two critical physical parameters:
- **Thermal Efficiency** (Î·)
- **Leakage Coefficient** (k)

## ğŸ”§ Installation

### On Local Machine
```bash
pip install -r requirements_pinn.txt
```

### On Azure VM
```bash
# Update system
sudo apt-get update
sudo apt-get install -y python3-pip python3-dev

# Install PyTorch (CPU version for Azure VMs without GPU)
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# Install other requirements
pip3 install -r requirements_pinn.txt

# Login to WandB
wandb login
```

## ğŸš€ Quick Start

### Basic Training Run
```bash
python train_pinn.py \
  --data_path furnace_data_cleaned.csv \
  --epochs 500 \
  --batch_size 64 \
  --lr 0.001 \
  --hidden_dim 64 \
  --layers 4 \
  --w_energy 0.1 \
  --w_mass 0.1
```

### With GPU (if available)
```bash
python train_pinn.py \
  --data_path furnace_data_cleaned.csv \
  --use_cuda \
  --batch_size 128 \
  --epochs 1000
```

### Custom Checkpoint Directory
```bash
python train_pinn.py \
  --data_path furnace_data_cleaned.csv \
  --checkpoint_dir ./my_experiments/run_001 \
  --log_dir ./my_experiments/logs
```

## ğŸ“Š WandB Sweeps (Hyperparameter Optimization)

### Initialize Sweep
```bash
# Create sweep
wandb sweep sweep_config.yaml

# Run sweep agent
wandb agent YOUR_SWEEP_ID
```

### Run Multiple Agents in Parallel
```bash
# Terminal 1
wandb agent YOUR_SWEEP_ID

# Terminal 2
wandb agent YOUR_SWEEP_ID

# Terminal 3
wandb agent YOUR_SWEEP_ID
```

## ğŸ—ï¸ Model Architecture

```
Input (6 features)
    â†“
[Linear â†’ Tanh] Ã— N layers (hidden_dim neurons each)
    â†“
Linear â†’ Output (2 predictions)
    â†“
[Outlet Temperature, Excess O2]

Learnable Parameters: Î· (efficiency), k (leakage)
```

## ğŸ“ˆ Loss Function

```
Total Loss = MSE(Data) + Î»â‚Â·MSE(Energy Balance) + Î»â‚‚Â·MSE(Mass Balance)

Where:
  Energy Balance: Q_in(Î·) = Q_out(T_out)
  Mass Balance:   O2 = f(k, Draft, Fuel)
```

## ğŸ” Monitoring Training

### Real-time Metrics (WandB)
- Training loss (data + physics)
- Validation MSE, RMSE, MAE, RÂ²
- Learned parameters (Î·, k) evolution
- Learning rate schedule

### Local Logs
```bash
tail -f logs/training_YYYYMMDD_HHMMSS.log
```

## ğŸ“ Output Structure

```
checkpoints/
  â”œâ”€â”€ best_pinn_model.pth          # Best model based on validation loss
  â”œâ”€â”€ checkpoint_epoch_100.pth     # Periodic checkpoints
  â”œâ”€â”€ checkpoint_epoch_200.pth
  â”œâ”€â”€ scaler.pkl                   # StandardScaler for inference
  â””â”€â”€ training_summary.json        # Final metrics & config

logs/
  â””â”€â”€ training_YYYYMMDD_HHMMSS.log # Detailed training log
```

## ğŸ›ï¸ Key Hyperparameters

| Parameter | Default | Description | Sweep Range |
|-----------|---------|-------------|-------------|
| `--lr` | 0.001 | Learning rate | 1e-4 to 1e-2 |
| `--hidden_dim` | 64 | Hidden layer size | 32, 64, 128, 256 |
| `--layers` | 4 | Number of layers | 3 to 6 |
| `--w_energy` | 0.1 | Energy physics weight | 0.01 to 1.0 |
| `--w_mass` | 0.1 | Mass physics weight | 0.01 to 1.0 |
| `--batch_size` | 64 | Training batch size | 32, 64, 128 |
| `--patience` | 50 | Early stopping patience | - |

## ğŸ§ª Physics Constraints

The model enforces:
1. **Energy Balance**: Heat input = Heat absorbed by process
2. **Mass Balance**: Air leakage affects oxygen concentration
3. **Clamping**: Prevents division by zero during low-flow transients
4. **Smooth Activations**: Tanh for continuous derivatives

## ğŸ“Š Expected Results

After training, typical performance:
- **Temperature RMSE**: 5-15 Â°C
- **O2 RMSE**: 0.5-2.0 %
- **RÂ² Score**: > 0.90
- **Learned Î·**: 0.60-0.75 (typical furnace efficiency)
- **Learned k**: 1.0-2.5 (leakage coefficient)

## ğŸ› Troubleshooting

### Loss is NaN
- Reduce learning rate: `--lr 0.0001`
- Increase gradient clipping: `--grad_clip 0.5`
- Reduce physics weights: `--w_energy 0.01 --w_mass 0.01`

### Poor Convergence
- Increase model capacity: `--hidden_dim 128 --layers 5`
- Adjust physics weights: `--w_energy 0.5 --w_mass 0.5`
- Enable learning rate scheduling (automatic)

### Out of Memory (OOM)
- Reduce batch size: `--batch_size 32`
- Reduce model size: `--hidden_dim 32 --layers 3`

## ğŸ“ Command Line Arguments

```bash
python train_pinn.py --help
```

## ğŸ”¬ For Thesis/Research

### Ablation Studies
```bash
# Data-only (no physics)
python train_pinn.py --w_energy 0.0 --w_mass 0.0

# Energy-only physics
python train_pinn.py --w_energy 0.5 --w_mass 0.0

# Mass-only physics
python train_pinn.py --w_energy 0.0 --w_mass 0.5

# Full PINN
python train_pinn.py --w_energy 0.5 --w_mass 0.5
```

### Reproducibility
All experiments are logged to WandB with full config for reproducibility.

## ğŸ“§ Support

For issues or questions, please check:
1. Training logs in `logs/` directory
2. WandB dashboard for metrics visualization
3. Model checkpoints in `checkpoints/` directory

## ğŸ“š Citation

If you use this code for research, please cite:
```
[Your thesis/paper details]
```
