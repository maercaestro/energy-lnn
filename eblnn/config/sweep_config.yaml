# W&B Sweep — Generative EB-LNN Hyperparameter Search
# =====================================================
# Searches over the key CD / Langevin hyper-parameters that have no
# equivalent in the pilot study (they did not exist there).

program: experiments/run_experiment.py
method: bayes           # Bayesian optimisation suits the continuous ranges
metric:
  name: val/loss
  goal: minimize

parameters:

  # ── Training balance ──────────────────────────────────────────────────
  alpha:
    values: [0.5, 1.0, 2.0, 5.0]   # L_CD weight

  l2_reg:
    values: [0.0, 0.005, 0.01, 0.05]

  learning_rate:
    values: [0.0001, 0.0005, 0.001, 0.005]

  # ── Model capacity ────────────────────────────────────────────────────
  hidden_size:
    values: [64, 128, 256]

  # ── EBM head depth ────────────────────────────────────────────────────
  # Note: expressed as a categorical choice; each string is parsed in runner
  ebm_hidden_dims:
    values: ["[64]", "[128, 64]", "[256, 128, 64]"]

  # ── Langevin dynamics ─────────────────────────────────────────────────
  n_steps:
    values: [10, 20, 50]

  step_size:
    values: [0.005, 0.01, 0.02]

  noise_scale:
    values: [0.001, 0.005, 0.01]
